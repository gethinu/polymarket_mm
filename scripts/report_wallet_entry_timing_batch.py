#!/usr/bin/env python3
"""
Batch timing report for candidate wallets.

Input:
  JSON generated by scripts/report_wallet_autopsy_candidates.py

Output:
  Aggregate timing report JSON under logs/ by default.
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import statistics
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.error import HTTPError, URLError
from urllib.parse import urlencode
from urllib.request import Request, urlopen


USER_AGENT = "Mozilla/5.0 (compatible; wallet-entry-timing-batch/1.0)"
DATA_API_BASE = "https://data-api.polymarket.com"
GAMMA_API_BASE = "https://gamma-api.polymarket.com"
DEFAULT_SIDES = "BUY"
DEFAULT_SEC_BUCKETS = "30,60,120,300,600,1800,3600,10800"
DEFAULT_TOP_MINUTES = 10

TIMING_PROFILES = {
    "none": {
        "sides": DEFAULT_SIDES,
        "sec_buckets": DEFAULT_SEC_BUCKETS,
        "top_minutes": DEFAULT_TOP_MINUTES,
    },
    "endgame_loose": {
        "sides": "BUY",
        "sec_buckets": "60,300,600,1800,3600,10800",
        "top_minutes": 10,
    },
    "endgame_consistent": {
        "sides": "BUY",
        "sec_buckets": "60,300,900,1800,3600,5400,10800",
        "top_minutes": 12,
    },
    "endgame_tight": {
        "sides": "BUY",
        "sec_buckets": "30,60,120,300,600,1200,1800,3600",
        "top_minutes": 15,
    },
}


def safe_print(msg: str) -> None:
    try:
        print(msg)
    except UnicodeEncodeError:
        try:
            enc = getattr(sys.stdout, "encoding", None) or "utf-8"
            print(msg.encode(enc, errors="replace").decode(enc, errors="replace"))
        except Exception:
            pass


def as_int(value, default: int = 0) -> int:
    try:
        return int(value)
    except (TypeError, ValueError):
        return default


def fetch_json(url: str, timeout_sec: float = 25.0, retries: int = 4):
    req = Request(url, headers={"User-Agent": USER_AGENT, "Accept": "application/json"})
    for i in range(retries):
        try:
            with urlopen(req, timeout=timeout_sec) as resp:
                return json.loads(resp.read().decode("utf-8"))
        except (HTTPError, URLError, TimeoutError, ValueError):
            if i >= retries - 1:
                return None
    return None


def parse_iso_to_ts(value: str) -> Optional[int]:
    if not value:
        return None
    try:
        return int(dt.datetime.fromisoformat(str(value).replace("Z", "+00:00")).timestamp())
    except Exception:
        return None


def fetch_market_end_ts(condition_id: str) -> Tuple[Optional[int], str]:
    if not condition_id:
        return None, ""
    q = urlencode({"condition_ids": condition_id})
    url = f"{GAMMA_API_BASE}/markets?{q}"
    data = fetch_json(url)
    if not isinstance(data, list) or not data:
        return None, ""
    end_iso = str(data[0].get("endDate") or "")
    return parse_iso_to_ts(end_iso), end_iso


def fetch_user_market_timestamps(wallet: str, condition_id: str, max_trades: int, sides: set[str]) -> List[int]:
    rows: List[int] = []
    limit = min(max(1, int(max_trades)), 500)
    offset = 0
    target = max(1, int(max_trades))
    while len(rows) < target:
        batch = min(limit, target - len(rows))
        q = urlencode({"user": wallet, "market": condition_id, "limit": str(batch), "offset": str(offset)})
        url = f"{DATA_API_BASE}/trades?{q}"
        data = fetch_json(url)
        if not isinstance(data, list) or not data:
            break
        for t in data:
            if not isinstance(t, dict):
                continue
            side = str(t.get("side") or "").upper()
            if sides and side not in sides:
                continue
            ts = as_int(t.get("timestamp"), 0)
            if ts > 0:
                rows.append(ts)
        if len(data) < batch:
            break
        offset += batch
    rows.sort()
    return rows


def median(values: List[float]) -> Optional[float]:
    if not values:
        return None
    vals = sorted(values)
    n = len(vals)
    m = n // 2
    if n % 2 == 1:
        return float(vals[m])
    return float((vals[m - 1] + vals[m]) / 2.0)


def percentile(values: List[float], p: float) -> Optional[float]:
    if not values:
        return None
    if len(values) == 1:
        return float(values[0])
    vals = sorted(values)
    pos = (len(vals) - 1) * p
    lo = int(pos)
    hi = min(lo + 1, len(vals) - 1)
    w = pos - lo
    return float(vals[lo] * (1 - w) + vals[hi] * w)


def bucket_label(low: Optional[int], high: Optional[int]) -> str:
    if low is None and high is None:
        return "all"
    if low is None:
        return f"<{high}s"
    if high is None:
        return f">={low}s"
    return f"{low}-{high}s"


def histogram(values: List[float], edges: List[int]) -> List[dict]:
    if not edges:
        return []
    bounds: List[Tuple[Optional[int], Optional[int]]] = []
    bounds.append((None, edges[0]))
    for i in range(len(edges) - 1):
        bounds.append((edges[i], edges[i + 1]))
    bounds.append((edges[-1], None))

    out = []
    for low, high in bounds:
        if low is None:
            cnt = sum(1 for v in values if v < float(high))
        elif high is None:
            cnt = sum(1 for v in values if v >= float(low))
        else:
            cnt = sum(1 for v in values if float(low) <= v < float(high))
        out.append({"bucket": bucket_label(low, high), "count": int(cnt)})
    return out


def parse_bucket_edges(raw: str) -> List[int]:
    out = []
    for tok in str(raw).split(","):
        tok = tok.strip()
        if not tok:
            continue
        try:
            v = int(tok)
            if v > 0:
                out.append(v)
        except ValueError:
            continue
    return sorted(set(out))


def repo_root() -> Path:
    return Path(__file__).resolve().parents[1]


def resolve_out_path(raw_out: str) -> Path:
    logs = repo_root() / "logs"
    logs.mkdir(parents=True, exist_ok=True)
    if not raw_out:
        return logs / "wallet_entry_timing_batch.json"
    p = Path(raw_out)
    if p.is_absolute():
        return p
    if len(p.parts) == 1:
        return logs / p.name
    return repo_root() / p


def load_candidate_rows(path: Path) -> List[dict]:
    with path.open("r", encoding="utf-8") as f:
        obj = json.load(f)
    if not isinstance(obj, dict):
        return []
    rows = obj.get("rows")
    if not isinstance(rows, list):
        return []
    return [r for r in rows if isinstance(r, dict)]


def hydrate_condition_from_source(row: dict) -> Tuple[str, str]:
    cond = str(row.get("condition_id") or "").strip()
    slug = str(row.get("market_slug") or "").strip()
    if cond:
        return cond, slug
    source = str(row.get("source_file") or "").strip()
    if not source:
        return "", slug
    p = Path(source)
    if not p.exists():
        return "", slug
    try:
        with p.open("r", encoding="utf-8") as f:
            obj = json.load(f)
        meta = obj.get("meta") if isinstance(obj.get("meta"), dict) else {}
        market = meta.get("market") if isinstance(meta.get("market"), dict) else {}
        cond2 = str(market.get("condition_id") or "").strip()
        slug2 = str(market.get("market_slug") or "").strip()
        return cond2, slug2 or slug
    except Exception:
        return "", slug


def main() -> int:
    p = argparse.ArgumentParser(description="Batch timing report from autopsy candidate JSON")
    p.add_argument("candidate_json", help="Input JSON generated by report_wallet_autopsy_candidates.py")
    p.add_argument(
        "--timing-profile",
        choices=["none", "endgame_loose", "endgame_consistent", "endgame_tight"],
        default="none",
        help="Preset timing display profile; explicit side/bucket/minute flags win when changed from defaults",
    )
    p.add_argument("--top", type=int, default=20, help="Max candidate rows to process")
    p.add_argument("--sides", default=DEFAULT_SIDES, help="Comma-separated sides for timing (BUY or BUY,SELL)")
    p.add_argument("--max-trades", type=int, default=1500, help="Max fetched trades per candidate")
    p.add_argument("--sec-buckets", default=DEFAULT_SEC_BUCKETS, help="Histogram bucket edges (seconds)")
    p.add_argument("--top-minutes", type=int, default=DEFAULT_TOP_MINUTES, help="Top minute-of-hour entries to keep")
    p.add_argument("--out", default="", help="Output JSON path (simple filename => logs/)")
    args = p.parse_args()

    if args.timing_profile != "none":
        prof = TIMING_PROFILES.get(args.timing_profile, TIMING_PROFILES["none"])
        if args.sides == DEFAULT_SIDES:
            args.sides = str(prof["sides"])
        if args.sec_buckets == DEFAULT_SEC_BUCKETS:
            args.sec_buckets = str(prof["sec_buckets"])
        if int(args.top_minutes) == int(DEFAULT_TOP_MINUTES):
            args.top_minutes = int(prof["top_minutes"])

    in_path = Path(args.candidate_json)
    if not in_path.exists():
        safe_print(f"Input not found: {in_path}")
        return 2

    rows = load_candidate_rows(in_path)
    if not rows:
        safe_print("No candidate rows found in input JSON.")
        return 1

    side_allow = {s.strip().upper() for s in str(args.sides).split(",") if s.strip()}
    edges = parse_bucket_edges(args.sec_buckets)
    top_n = max(1, int(args.top))
    selected = rows[:top_n]

    end_cache: Dict[str, Tuple[Optional[int], str]] = {}
    out_rows = []
    safe_print(f"Input candidates: {len(rows)}")
    safe_print(f"Processing top:   {len(selected)}")

    for i, row in enumerate(selected, start=1):
        wallet = str(row.get("wallet") or "").strip().lower()
        market = str(row.get("market") or "").strip()
        cond, slug = hydrate_condition_from_source(row)
        if not wallet or not cond:
            safe_print(f"[{i}/{len(selected)}] skipped (missing wallet/condition): {wallet} | {market}")
            continue

        end_pair = end_cache.get(cond)
        if end_pair is None:
            end_pair = fetch_market_end_ts(cond)
            end_cache[cond] = end_pair
        end_ts, end_iso = end_pair
        if end_ts is None:
            safe_print(f"[{i}/{len(selected)}] skipped (no endDate): {wallet} | {market}")
            continue

        ts_rows = fetch_user_market_timestamps(
            wallet=wallet,
            condition_id=cond,
            max_trades=int(args.max_trades),
            sides=side_allow,
        )
        if not ts_rows:
            safe_print(f"[{i}/{len(selected)}] skipped (no trades): {wallet} | {market}")
            continue

        intervals = [
            float(ts_rows[j] - ts_rows[j - 1])
            for j in range(1, len(ts_rows))
            if ts_rows[j] >= ts_rows[j - 1]
        ]
        sec_to_end = [float(end_ts - ts) for ts in ts_rows]

        minute_counts: Dict[int, int] = {}
        for ts in ts_rows:
            m = dt.datetime.fromtimestamp(ts, tz=dt.timezone.utc).minute
            minute_counts[m] = minute_counts.get(m, 0) + 1
        minute_rank = sorted(minute_counts.items(), key=lambda kv: kv[1], reverse=True)[: max(1, int(args.top_minutes))]

        rep = {
            "wallet": wallet,
            "market": market,
            "condition_id": cond,
            "market_slug": slug,
            "trade_count_included": len(ts_rows),
            "included_sides": sorted(list(side_allow)),
            "first_trade_ts": ts_rows[0],
            "last_trade_ts": ts_rows[-1],
            "end_date_iso": end_iso,
            "avg_interval_sec": statistics.mean(intervals) if intervals else 0.0,
            "median_interval_sec": median(intervals) if intervals else 0.0,
            "time_to_end_sec_p10": percentile(sec_to_end, 0.10),
            "time_to_end_sec_p50": median(sec_to_end),
            "time_to_end_sec_p90": percentile(sec_to_end, 0.90),
            "time_to_end_histogram": histogram(sec_to_end, edges) if edges else [],
            "minute_of_hour_top": [{"minute_utc": int(m), "count": int(c)} for m, c in minute_rank],
        }
        out_rows.append(rep)
        p50 = rep.get("time_to_end_sec_p50")
        p50_s = f"{float(p50):.0f}" if p50 is not None else "-"
        safe_print(f"[{i}/{len(selected)}] ok trades={len(ts_rows):4d} tte_p50={p50_s:>6}s wallet={wallet}")

    out_rows.sort(key=lambda r: float(r.get("time_to_end_sec_p50") or 1e18))
    safe_print("-" * 90)
    safe_print(f"{'tte_p50':>9} {'trades':>7} {'wallet':<42} market")
    for r in out_rows:
        p50 = r.get("time_to_end_sec_p50")
        p50_s = f"{float(p50):.0f}" if p50 is not None else "-"
        safe_print(f"{p50_s:>9} {int(r.get('trade_count_included') or 0):>7} {str(r.get('wallet') or ''):<42} {str(r.get('market') or '')}")

    out_path = resolve_out_path(args.out)
    payload = {
        "meta": {
            "generated_at_utc": dt.datetime.now(dt.timezone.utc).isoformat(),
            "source_candidate_json": str(in_path),
            "candidate_rows_input": len(rows),
            "candidate_rows_processed": len(selected),
            "timing_rows_output": len(out_rows),
            "included_sides": sorted(list(side_allow)),
            "max_trades": int(args.max_trades),
            "sec_buckets": edges,
            "top_minutes": int(args.top_minutes),
            "timing_profile": str(args.timing_profile),
        },
        "rows": out_rows,
    }
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    safe_print("")
    safe_print(f"Saved: {out_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
